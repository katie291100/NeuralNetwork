{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "Copy of INT2_Group2.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "%%capture\n",
    "%%bash\n",
    "pip install captum\n",
    "pip install flask_compress\n",
    "conda install freetype=2.10.4"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# default `log_dir` is \"runs\" - we'll be more specific here\n",
    "writer = SummaryWriter('runs/first_visualization_test')"
   ],
   "metadata": {
    "id": "TNY_v8m0ih6G",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 14,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from captum.attr import IntegratedGradients\n",
    "from captum.attr import LayerConductance\n",
    "from captum.attr import NeuronConductance\n",
    "from captum.insights import AttributionVisualizer, Batch\n",
    "from captum.insights.attr_vis.features import ImageFeature\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time"
   ],
   "metadata": {
    "id": "BwDdxZ-LiiKI",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 15,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "if torch.cuda.is_available(): # use gpu if possible\n",
    "  device = torch.device(\"cuda\")\n",
    "else:\n",
    "  device = torch.device(\"cpu\")"
   ],
   "metadata": {
    "id": "JR6ZVgNKiiNH",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 16,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "epochs = 4000\n",
    "batch_size = 400\n",
    "learning_rate = 0.00015\n",
    "\n",
    "# only need resize so AlexNet works\n",
    "train_transform = transforms.Compose([transforms.ToTensor(), \n",
    "                                transforms.RandomHorizontalFlip(),\n",
    "                                transforms.RandomAffine(0, (0.1, 0.1)),\n",
    "                                transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])\n",
    "\n",
    "test_transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])"
   ],
   "metadata": {
    "id": "GZ1U8fyHiiQ4",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 17,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# get training/test data from CIFAR10 dataset\n",
    "train_data = torchvision.datasets.CIFAR10(root = \"./dataset/train\",\n",
    "                                        train = True, \n",
    "                                        transform = train_transform, \n",
    "                                        download = True)\n",
    "test_data = torchvision.datasets.CIFAR10(root = \"./dataset/test\",\n",
    "                                       train = False, \n",
    "                                       transform = test_transform, \n",
    "                                       download = True)"
   ],
   "metadata": {
    "id": "VPf031VViiWo",
    "outputId": "d08ede15-07f2-4a78-8988-b19ed3a61767",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset = train_data, \n",
    "                                           batch_size = batch_size, \n",
    "                                           shuffle = True, \n",
    "                                           num_workers = 3)\n",
    "test_loader = torch.utils.data.DataLoader(dataset = test_data, \n",
    "                                          batch_size = batch_size, \n",
    "                                          shuffle = False, \n",
    "                                          num_workers = 3)"
   ],
   "metadata": {
    "id": "yzpr2U31iiZo",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 19,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class CNN(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(CNN, self).__init__()\n",
    "    \n",
    "    self.conv1 = nn.Conv2d(3, 128, 3, 1)\n",
    "    self.conv2 = nn.Conv2d(128, 128, 3, 1, padding=2)\n",
    "    self.conv3 = nn.Conv2d(128, 128, 3, 1, padding=2)\n",
    "    self.conv4 = nn.Conv2d(128, 128, 3, 1, padding=1)\n",
    "    self.conv5 = nn.Conv2d(128, 128, 3, 1, padding=1)\n",
    "    self.conv6 = nn.Conv2d(128, 128, 3, 1)\n",
    "    self.conv7 = nn.Conv2d(128, 256, 3, 1)\n",
    "\n",
    "    self.maxPool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "    self.drop1 = nn.Dropout(0.3)\n",
    "    self.drop2 = nn.Dropout(0.3)\n",
    "    self.drop3 = nn.Dropout(0.2)\n",
    "    self.drop4 = nn.Dropout(0.4)\n",
    "\n",
    "    self.BN1 = nn.BatchNorm2d(128)\n",
    "    self.BN2 = nn.BatchNorm2d(128)\n",
    "    self.BN3 = nn.BatchNorm2d(128)\n",
    "\n",
    "    self.fc1 = nn.Linear(256, 128)\n",
    "    self.fc2 = nn.Linear(128, 64)\n",
    "    self.fc3 = nn.Linear(64, 10)\n",
    "  \n",
    "  def forward(self, x):\n",
    "    x = nn.functional.rrelu(self.conv1(x))\n",
    "    x = self.maxPool(x)\n",
    "    x = self.drop1(x)\n",
    "    x = nn.functional.rrelu(self.conv2(x))\n",
    "    x = self.maxPool(x)\n",
    "    x = self.drop2(x)\n",
    "\n",
    "    x = self.BN1(x)\n",
    "\n",
    "    x = nn.functional.rrelu(self.conv3(x))\n",
    "\n",
    "    x = nn.functional.rrelu(self.conv4(x))\n",
    "    x = self.maxPool(x)\n",
    "    x = self.drop3(x)\n",
    "\n",
    "    x = nn.functional.rrelu(self.conv5(x))\n",
    "    x = self.BN2(x)\n",
    "    x = nn.functional.rrelu(self.conv6(x))\n",
    "    x = self.drop4(x)\n",
    "  # x = self.BN3(x)\n",
    "\n",
    "    x = nn.functional.rrelu(self.conv7(x))\n",
    "\n",
    "    x = x.reshape(x.shape[0], -1)\n",
    "    #print(x.shape)\n",
    "    x = nn.functional.rrelu(self.fc1(x))\n",
    "    x = nn.functional.rrelu(self.fc2(x))\n",
    "    x = self.fc3(x)\n",
    "    return x\n",
    "  "
   ],
   "metadata": {
    "id": "12r09e6KiifY",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 20,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Model creation\n",
    "\n",
    "model_copy = CNN()\n",
    "model = CNN().to(device)"
   ],
   "metadata": {
    "id": "9zrgyIqEiiia",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 21,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def matplotlib_imshow(img, one_channel=False):\n",
    "    if one_channel:\n",
    "        img = img.mean(dim=0)\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    try:\n",
    "      npimg = img.numpy()\n",
    "    except:\n",
    "      npimg = img.cpu().data.numpy()\n",
    "    if one_channel:\n",
    "        plt.imshow(npimg, cmap=\"Greys\")\n",
    "    else:\n",
    "        plt.imshow(np.transpose(npimg, (1, 2, 0)))"
   ],
   "metadata": {
    "id": "-bgHqO1Biik6",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 22,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# helper functions\n",
    "\n",
    "def images_to_probs(net, images):\n",
    "    '''\n",
    "    Generates predictions and corresponding probabilities from a trained\n",
    "    network and a list of images\n",
    "    '''\n",
    "    output = net(images)\n",
    "    # convert output probabilities to predicted class\n",
    "    _, preds_tensor = torch.max(output, 1)\n",
    "    preds = np.squeeze(preds_tensor.cpu().data.numpy())\n",
    "    return preds, [nn.functional.softmax(el, dim=0)[i].item() for i, el in zip(preds, output)]\n",
    "\n",
    "\n",
    "def plot_classes_preds(net, images, labels):\n",
    "    '''\n",
    "    Generates matplotlib Figure using a trained network, along with images\n",
    "    and labels from a batch, that shows the network's top prediction along\n",
    "    with its probability, alongside the actual label, coloring this\n",
    "    information based on whether the prediction was correct or not.\n",
    "    Uses the \"images_to_probs\" function.\n",
    "    '''\n",
    "    preds, probs = images_to_probs(net, images)\n",
    "    # plot the images in the batch, along with predicted and true labels\n",
    "    fig = plt.figure(figsize=(12, 48))\n",
    "    for idx in np.arange(4):\n",
    "        ax = fig.add_subplot(1, 4, idx+1, xticks=[], yticks=[])\n",
    "        matplotlib_imshow(images[idx], one_channel=True)\n",
    "        ax.set_title(\"{0}, {1:.1f}%\\n(label: {2})\".format(\n",
    "            train_data.classes[preds[idx]],\n",
    "            probs[idx] * 100.0,\n",
    "            train_data.classes[labels[idx]]),\n",
    "                    color=(\"green\" if preds[idx]==labels[idx].item() else \"red\"))\n",
    "    return fig\n"
   ],
   "metadata": {
    "id": "GH4m_yhaiinq",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 23,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "def test():\n",
    "  print(\"Testing\")\n",
    "  with torch.no_grad():\n",
    "    correct = 0\n",
    "    samples = 0\n",
    "\n",
    "    for (images, labels) in test_loader:\n",
    "      images = images.to(device)\n",
    "      labels = labels.to(device)\n",
    "\n",
    "      outputs = model(images)\n",
    "      _, predictions = outputs.max(1)\n",
    "\n",
    "      samples += labels.size(0)\n",
    "      correct += (predictions == labels).sum()\n",
    "\n",
    "    print(\"Test accuracy was\",100*float(correct)/float(samples))\n",
    "    print()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "## Training\n",
    "\n",
    "# try other loss functions/optimizers\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), learning_rate, weight_decay=0.0015)\n",
    "\n",
    "test_per_epoch = True\n",
    "train_for_time = 0 # how many minutes to train for (and then finish current epoch)\n",
    "\n",
    "if train_for_time:\n",
    "  epochs = train_for_time*1000\n",
    "start = time.time()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "  print(\"Training\")\n",
    "  epoch_loss = 0\n",
    "  previous_loss = 0\n",
    "  for i, (images, labels) in enumerate(train_loader):\n",
    "    images = images.to(device)\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    # forwards\n",
    "    outputs = model(images)\n",
    "    loss = criterion(outputs, labels)\n",
    "\n",
    "    # backwards\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    epoch_loss += loss.item()\n",
    "\n",
    "    # if i % 1000 == 999:    # every 1000 mini-batches...\n",
    "    #\n",
    "    #         # ...log the running loss\n",
    "    #         writer.add_scalar('training loss',\n",
    "    #                         epoch_loss-previous_loss / 1000,\n",
    "    #                         epoch * len(train_loader) + i)\n",
    "    #\n",
    "    #         # ...log a Matplotlib Figure showing the model's predictions on a\n",
    "    #         # random mini-batch\n",
    "    #         writer.add_figure('predictions vs. actuals',\n",
    "    #                         plot_classes_preds(model, images, labels),\n",
    "    #                         global_step=epoch * len(train_loader) + i)\n",
    "    #         previous_loss = epoch_loss\n",
    "  \n",
    "  print(\"Epoch\", epoch+1, \"complete\")\n",
    "  print(\"Loss was\", epoch_loss/len(train_loader))\n",
    "  print()\n",
    "  if epoch%10 == 0:\n",
    "    test()\n",
    "  \n",
    "  if train_for_time and time.time()-start >= train_for_time*60:\n",
    "    break\n",
    "\n",
    "if not test_per_epoch:\n",
    "  test()"
   ],
   "metadata": {
    "id": "byaDzD35itRL",
    "outputId": "2ed84bcb-89cd-4d21-ea07-66a0c2779bc3",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Epoch 1 complete\n",
      "Loss was 1.8043572006225586\n",
      "\n",
      "Testing\n",
      "Test accuracy was 43.84\n",
      "\n",
      "Training\n",
      "Epoch 2 complete\n",
      "Loss was 1.4276155281066893\n",
      "\n",
      "Training\n",
      "Epoch 3 complete\n",
      "Loss was 1.2650232105255126\n",
      "\n",
      "Training\n",
      "Epoch 4 complete\n",
      "Loss was 1.1444818325042725\n",
      "\n",
      "Training\n",
      "Epoch 5 complete\n",
      "Loss was 1.03894819355011\n",
      "\n",
      "Training\n",
      "Epoch 6 complete\n",
      "Loss was 0.9596415576934815\n",
      "\n",
      "Training\n",
      "Epoch 7 complete\n",
      "Loss was 0.9035965647697449\n",
      "\n",
      "Training\n",
      "Epoch 8 complete\n",
      "Loss was 0.850675518989563\n",
      "\n",
      "Training\n",
      "Epoch 9 complete\n",
      "Loss was 0.809143618106842\n",
      "\n",
      "Training\n",
      "Epoch 10 complete\n",
      "Loss was 0.7796392693519593\n",
      "\n",
      "Training\n",
      "Epoch 11 complete\n",
      "Loss was 0.7507971234321594\n",
      "\n",
      "Testing\n",
      "Test accuracy was 74.15\n",
      "\n",
      "Training\n",
      "Epoch 12 complete\n",
      "Loss was 0.7274836087226868\n",
      "\n",
      "Training\n",
      "Epoch 13 complete\n",
      "Loss was 0.704644124507904\n",
      "\n",
      "Training\n",
      "Epoch 14 complete\n",
      "Loss was 0.6763110361099243\n",
      "\n",
      "Training\n",
      "Epoch 15 complete\n",
      "Loss was 0.6612437267303467\n",
      "\n",
      "Training\n",
      "Epoch 16 complete\n",
      "Loss was 0.6552680625915527\n",
      "\n",
      "Training\n",
      "Epoch 17 complete\n",
      "Loss was 0.6305493664741516\n",
      "\n",
      "Training\n",
      "Epoch 18 complete\n",
      "Loss was 0.6167526760101318\n",
      "\n",
      "Training\n",
      "Epoch 19 complete\n",
      "Loss was 0.60455335855484\n",
      "\n",
      "Training\n",
      "Epoch 20 complete\n",
      "Loss was 0.5879678225517273\n",
      "\n",
      "Training\n",
      "Epoch 21 complete\n",
      "Loss was 0.5818128304481507\n",
      "\n",
      "Testing\n",
      "Test accuracy was 78.55\n",
      "\n",
      "Training\n",
      "Epoch 22 complete\n",
      "Loss was 0.5720191493034362\n",
      "\n",
      "Training\n",
      "Epoch 23 complete\n",
      "Loss was 0.5622562663555145\n",
      "\n",
      "Training\n",
      "Epoch 24 complete\n",
      "Loss was 0.5466370856761933\n",
      "\n",
      "Training\n",
      "Epoch 25 complete\n",
      "Loss was 0.5474614570140839\n",
      "\n",
      "Training\n",
      "Epoch 26 complete\n",
      "Loss was 0.5328238635063172\n",
      "\n",
      "Training\n",
      "Epoch 27 complete\n",
      "Loss was 0.525118246793747\n",
      "\n",
      "Training\n",
      "Epoch 28 complete\n",
      "Loss was 0.5163797409534454\n",
      "\n",
      "Training\n",
      "Epoch 29 complete\n",
      "Loss was 0.5125685794353485\n",
      "\n",
      "Training\n",
      "Epoch 30 complete\n",
      "Loss was 0.4987551553249359\n",
      "\n",
      "Training\n",
      "Epoch 31 complete\n",
      "Loss was 0.4956264328956604\n",
      "\n",
      "Testing\n",
      "Test accuracy was 80.86\n",
      "\n",
      "Training\n",
      "Epoch 32 complete\n",
      "Loss was 0.49006292963027953\n",
      "\n",
      "Training\n",
      "Epoch 33 complete\n",
      "Loss was 0.4852608897686005\n",
      "\n",
      "Training\n",
      "Epoch 34 complete\n",
      "Loss was 0.4701807408332825\n",
      "\n",
      "Training\n",
      "Epoch 35 complete\n",
      "Loss was 0.473353798866272\n",
      "\n",
      "Training\n",
      "Epoch 36 complete\n",
      "Loss was 0.46316714835166933\n",
      "\n",
      "Training\n",
      "Epoch 37 complete\n",
      "Loss was 0.46194208002090453\n",
      "\n",
      "Training\n",
      "Epoch 38 complete\n",
      "Loss was 0.45173209357261657\n",
      "\n",
      "Training\n",
      "Epoch 39 complete\n",
      "Loss was 0.4479074218273163\n",
      "\n",
      "Training\n",
      "Epoch 40 complete\n",
      "Loss was 0.43990720582008364\n",
      "\n",
      "Training\n",
      "Epoch 41 complete\n",
      "Loss was 0.4438803923130035\n",
      "\n",
      "Testing\n",
      "Test accuracy was 81.61\n",
      "\n",
      "Training\n",
      "Epoch 42 complete\n",
      "Loss was 0.43077470231056214\n",
      "\n",
      "Training\n",
      "Epoch 43 complete\n",
      "Loss was 0.4271643621921539\n",
      "\n",
      "Training\n",
      "Epoch 44 complete\n",
      "Loss was 0.42488491559028624\n",
      "\n",
      "Training\n",
      "Epoch 45 complete\n",
      "Loss was 0.41844713425636293\n",
      "\n",
      "Training\n",
      "Epoch 46 complete\n",
      "Loss was 0.41474653959274294\n",
      "\n",
      "Training\n",
      "Epoch 47 complete\n",
      "Loss was 0.41327436423301694\n",
      "\n",
      "Training\n",
      "Epoch 48 complete\n",
      "Loss was 0.40311229515075686\n",
      "\n",
      "Training\n",
      "Epoch 49 complete\n",
      "Loss was 0.4032249116897583\n",
      "\n",
      "Training\n",
      "Epoch 50 complete\n",
      "Loss was 0.4024148721694946\n",
      "\n",
      "Training\n",
      "Epoch 51 complete\n",
      "Loss was 0.3914669740200043\n",
      "\n",
      "Testing\n",
      "Test accuracy was 82.46\n",
      "\n",
      "Training\n",
      "Epoch 52 complete\n",
      "Loss was 0.393478125333786\n",
      "\n",
      "Training\n",
      "Epoch 53 complete\n",
      "Loss was 0.38969929671287534\n",
      "\n",
      "Training\n",
      "Epoch 54 complete\n",
      "Loss was 0.3775941607952118\n",
      "\n",
      "Training\n",
      "Epoch 55 complete\n",
      "Loss was 0.385361617565155\n",
      "\n",
      "Training\n",
      "Epoch 56 complete\n",
      "Loss was 0.38175290513038634\n",
      "\n",
      "Training\n",
      "Epoch 57 complete\n",
      "Loss was 0.3780011520385742\n",
      "\n",
      "Training\n",
      "Epoch 58 complete\n",
      "Loss was 0.36968255519866944\n",
      "\n",
      "Training\n",
      "Epoch 59 complete\n",
      "Loss was 0.3705096652507782\n",
      "\n",
      "Training\n",
      "Epoch 60 complete\n",
      "Loss was 0.36259945440292357\n",
      "\n",
      "Training\n",
      "Epoch 61 complete\n",
      "Loss was 0.3615826892852783\n",
      "\n",
      "Testing\n",
      "Test accuracy was 84.42\n",
      "\n",
      "Training\n",
      "Epoch 62 complete\n",
      "Loss was 0.3576006548404694\n",
      "\n",
      "Training\n",
      "Epoch 63 complete\n",
      "Loss was 0.35914992046356203\n",
      "\n",
      "Training\n",
      "Epoch 64 complete\n",
      "Loss was 0.35477525305747987\n",
      "\n",
      "Training\n",
      "Epoch 65 complete\n",
      "Loss was 0.35570964765548707\n",
      "\n",
      "Training\n",
      "Epoch 66 complete\n",
      "Loss was 0.3528512715101242\n",
      "\n",
      "Training\n",
      "Epoch 67 complete\n",
      "Loss was 0.34726144886016846\n",
      "\n",
      "Training\n",
      "Epoch 68 complete\n",
      "Loss was 0.3464026856422424\n",
      "\n",
      "Training\n",
      "Epoch 69 complete\n",
      "Loss was 0.34284348320961\n",
      "\n",
      "Training\n",
      "Epoch 70 complete\n",
      "Loss was 0.33757719135284425\n",
      "\n",
      "Training\n",
      "Epoch 71 complete\n",
      "Loss was 0.3347731446027756\n",
      "\n",
      "Testing\n",
      "Test accuracy was 84.42\n",
      "\n",
      "Training\n",
      "Epoch 72 complete\n",
      "Loss was 0.3350503706932068\n",
      "\n",
      "Training\n",
      "Epoch 73 complete\n",
      "Loss was 0.3362115845680237\n",
      "\n",
      "Training\n",
      "Epoch 74 complete\n",
      "Loss was 0.330202840089798\n",
      "\n",
      "Training\n",
      "Epoch 75 complete\n",
      "Loss was 0.3312844396829605\n",
      "\n",
      "Training\n",
      "Epoch 76 complete\n",
      "Loss was 0.32993265438079833\n",
      "\n",
      "Training\n",
      "Epoch 77 complete\n",
      "Loss was 0.32587357723712923\n",
      "\n",
      "Training\n",
      "Epoch 78 complete\n",
      "Loss was 0.3193807451725006\n",
      "\n",
      "Training\n",
      "Epoch 79 complete\n",
      "Loss was 0.3192047704458237\n",
      "\n",
      "Training\n",
      "Epoch 80 complete\n",
      "Loss was 0.3218246614933014\n",
      "\n",
      "Training\n",
      "Epoch 81 complete\n",
      "Loss was 0.31271891176700595\n",
      "\n",
      "Testing\n",
      "Test accuracy was 84.5\n",
      "\n",
      "Training\n",
      "Epoch 82 complete\n",
      "Loss was 0.31644290447235107\n",
      "\n",
      "Training\n",
      "Epoch 83 complete\n",
      "Loss was 0.31345727503299714\n",
      "\n",
      "Training\n",
      "Epoch 84 complete\n",
      "Loss was 0.3140319349765778\n",
      "\n",
      "Training\n",
      "Epoch 85 complete\n",
      "Loss was 0.31258191192150114\n",
      "\n",
      "Training\n",
      "Epoch 86 complete\n",
      "Loss was 0.30824047422409057\n",
      "\n",
      "Training\n",
      "Epoch 87 complete\n",
      "Loss was 0.301592621088028\n",
      "\n",
      "Training\n",
      "Epoch 88 complete\n",
      "Loss was 0.30707674407958985\n",
      "\n",
      "Training\n",
      "Epoch 89 complete\n",
      "Loss was 0.30305370008945465\n",
      "\n",
      "Training\n",
      "Epoch 90 complete\n",
      "Loss was 0.30242883777618407\n",
      "\n",
      "Training\n",
      "Epoch 91 complete\n",
      "Loss was 0.2977633628845215\n",
      "\n",
      "Testing\n",
      "Test accuracy was 85.36\n",
      "\n",
      "Training\n",
      "Epoch 92 complete\n",
      "Loss was 0.30129193449020386\n",
      "\n",
      "Training\n",
      "Epoch 93 complete\n",
      "Loss was 0.2960081701278687\n",
      "\n",
      "Training\n",
      "Epoch 94 complete\n",
      "Loss was 0.2969589115381241\n",
      "\n",
      "Training\n",
      "Epoch 95 complete\n",
      "Loss was 0.2922121750116348\n",
      "\n",
      "Training\n",
      "Epoch 96 complete\n",
      "Loss was 0.29481818568706514\n",
      "\n",
      "Training\n",
      "Epoch 97 complete\n",
      "Loss was 0.29112724697589876\n",
      "\n",
      "Training\n",
      "Epoch 98 complete\n",
      "Loss was 0.29501764702796934\n",
      "\n",
      "Training\n",
      "Epoch 99 complete\n",
      "Loss was 0.28918925642967225\n",
      "\n",
      "Training\n",
      "Epoch 100 complete\n",
      "Loss was 0.28631620907783506\n",
      "\n",
      "Training\n",
      "Epoch 101 complete\n",
      "Loss was 0.2821528607606888\n",
      "\n",
      "Testing\n",
      "Test accuracy was 85.54\n",
      "\n",
      "Training\n",
      "Epoch 102 complete\n",
      "Loss was 0.28688424932956696\n",
      "\n",
      "Training\n",
      "Epoch 103 complete\n",
      "Loss was 0.2798560996055603\n",
      "\n",
      "Training\n",
      "Epoch 104 complete\n",
      "Loss was 0.27695612955093385\n",
      "\n",
      "Training\n",
      "Epoch 105 complete\n",
      "Loss was 0.28275939416885376\n",
      "\n",
      "Training\n",
      "Epoch 106 complete\n",
      "Loss was 0.27963717830181123\n",
      "\n",
      "Training\n",
      "Epoch 107 complete\n",
      "Loss was 0.2772314692735672\n",
      "\n",
      "Training\n",
      "Epoch 108 complete\n",
      "Loss was 0.2772824590206146\n",
      "\n",
      "Training\n",
      "Epoch 109 complete\n",
      "Loss was 0.274818302154541\n",
      "\n",
      "Training\n",
      "Epoch 110 complete\n",
      "Loss was 0.27467822122573854\n",
      "\n",
      "Training\n",
      "Epoch 111 complete\n",
      "Loss was 0.27274779880046846\n",
      "\n",
      "Testing\n",
      "Test accuracy was 85.18\n",
      "\n",
      "Training\n",
      "Epoch 112 complete\n",
      "Loss was 0.277146071434021\n",
      "\n",
      "Training\n",
      "Epoch 113 complete\n",
      "Loss was 0.2721937849521637\n",
      "\n",
      "Training\n",
      "Epoch 114 complete\n",
      "Loss was 0.2714121334552765\n",
      "\n",
      "Training\n",
      "Epoch 115 complete\n",
      "Loss was 0.267133109331131\n",
      "\n",
      "Training\n",
      "Epoch 116 complete\n",
      "Loss was 0.26735358691215516\n",
      "\n",
      "Training\n",
      "Epoch 117 complete\n",
      "Loss was 0.26817898309230803\n",
      "\n",
      "Training\n",
      "Epoch 118 complete\n",
      "Loss was 0.26385766112804415\n",
      "\n",
      "Training\n",
      "Epoch 119 complete\n",
      "Loss was 0.2596216809749603\n",
      "\n",
      "Training\n",
      "Epoch 120 complete\n",
      "Loss was 0.2623328560590744\n",
      "\n",
      "Training\n",
      "Epoch 121 complete\n",
      "Loss was 0.263958135843277\n",
      "\n",
      "Testing\n",
      "Test accuracy was 85.41\n",
      "\n",
      "Training\n",
      "Epoch 122 complete\n",
      "Loss was 0.2562297601699829\n",
      "\n",
      "Training\n",
      "Epoch 123 complete\n",
      "Loss was 0.2635052757263184\n",
      "\n",
      "Training\n",
      "Epoch 124 complete\n",
      "Loss was 0.26029704332351683\n",
      "\n",
      "Training\n",
      "Epoch 125 complete\n",
      "Loss was 0.2601229839324951\n",
      "\n",
      "Training\n",
      "Epoch 126 complete\n",
      "Loss was 0.2649789011478424\n",
      "\n",
      "Training\n",
      "Epoch 127 complete\n",
      "Loss was 0.25930189871788023\n",
      "\n",
      "Training\n",
      "Epoch 128 complete\n",
      "Loss was 0.2572838315963745\n",
      "\n",
      "Training\n",
      "Epoch 129 complete\n",
      "Loss was 0.2548300186395645\n",
      "\n",
      "Training\n",
      "Epoch 130 complete\n",
      "Loss was 0.2554499192237854\n",
      "\n",
      "Training\n",
      "Epoch 131 complete\n",
      "Loss was 0.24965778172016143\n",
      "\n",
      "Testing\n",
      "Test accuracy was 85.94\n",
      "\n",
      "Training\n",
      "Epoch 132 complete\n",
      "Loss was 0.2540007412433624\n",
      "\n",
      "Training\n",
      "Epoch 133 complete\n",
      "Loss was 0.25194020044803617\n",
      "\n",
      "Training\n",
      "Epoch 134 complete\n",
      "Loss was 0.25224286806583407\n",
      "\n",
      "Training\n",
      "Epoch 135 complete\n",
      "Loss was 0.24917340779304503\n",
      "\n",
      "Training\n",
      "Epoch 136 complete\n",
      "Loss was 0.24750545537471771\n",
      "\n",
      "Training\n",
      "Epoch 137 complete\n",
      "Loss was 0.2486737368106842\n",
      "\n",
      "Training\n",
      "Epoch 138 complete\n",
      "Loss was 0.25118093204498293\n",
      "\n",
      "Training\n",
      "Epoch 139 complete\n",
      "Loss was 0.2431987589597702\n",
      "\n",
      "Training\n",
      "Epoch 140 complete\n",
      "Loss was 0.250262030005455\n",
      "\n",
      "Training\n",
      "Epoch 141 complete\n",
      "Loss was 0.24091662800312041\n",
      "\n",
      "Testing\n",
      "Test accuracy was 85.64\n",
      "\n",
      "Training\n",
      "Epoch 142 complete\n",
      "Loss was 0.25062474834918974\n",
      "\n",
      "Training\n",
      "Epoch 143 complete\n",
      "Loss was 0.24630311524868012\n",
      "\n",
      "Training\n",
      "Epoch 144 complete\n",
      "Loss was 0.24237665581703186\n",
      "\n",
      "Training\n",
      "Epoch 145 complete\n",
      "Loss was 0.2433036720752716\n",
      "\n",
      "Training\n",
      "Epoch 146 complete\n",
      "Loss was 0.24123999607563018\n",
      "\n",
      "Training\n",
      "Epoch 147 complete\n",
      "Loss was 0.24159809458255768\n",
      "\n",
      "Training\n",
      "Epoch 148 complete\n",
      "Loss was 0.23925410962104798\n",
      "\n",
      "Training\n",
      "Epoch 149 complete\n",
      "Loss was 0.23397598624229432\n",
      "\n",
      "Training\n",
      "Epoch 150 complete\n",
      "Loss was 0.24338166105747222\n",
      "\n",
      "Training\n",
      "Epoch 151 complete\n",
      "Loss was 0.24216074335575105\n",
      "\n",
      "Testing\n",
      "Test accuracy was 86.27\n",
      "\n",
      "Training\n",
      "Epoch 152 complete\n",
      "Loss was 0.24073695349693297\n",
      "\n",
      "Training\n",
      "Epoch 153 complete\n",
      "Loss was 0.23875737571716307\n",
      "\n",
      "Training\n",
      "Epoch 154 complete\n",
      "Loss was 0.24066447126865387\n",
      "\n",
      "Training\n",
      "Epoch 155 complete\n",
      "Loss was 0.23640261101722718\n",
      "\n",
      "Training\n",
      "Epoch 156 complete\n",
      "Loss was 0.23897895097732544\n",
      "\n",
      "Training\n",
      "Epoch 157 complete\n",
      "Loss was 0.2385817105770111\n",
      "\n",
      "Training\n",
      "Epoch 158 complete\n",
      "Loss was 0.23289756298065187\n",
      "\n",
      "Training\n",
      "Epoch 159 complete\n",
      "Loss was 0.23650419282913207\n",
      "\n",
      "Training\n",
      "Epoch 160 complete\n",
      "Loss was 0.2368624805212021\n",
      "\n",
      "Training\n",
      "Epoch 161 complete\n",
      "Loss was 0.2306403741836548\n",
      "\n",
      "Testing\n",
      "Test accuracy was 86.2\n",
      "\n",
      "Training\n",
      "Epoch 162 complete\n",
      "Loss was 0.22828497588634492\n",
      "\n",
      "Training\n",
      "Epoch 163 complete\n",
      "Loss was 0.22669341337680818\n",
      "\n",
      "Training\n",
      "Epoch 164 complete\n",
      "Loss was 0.2296007616519928\n",
      "\n",
      "Training\n",
      "Epoch 165 complete\n",
      "Loss was 0.23205607211589813\n",
      "\n",
      "Training\n",
      "Epoch 166 complete\n",
      "Loss was 0.2310113000869751\n",
      "\n",
      "Training\n",
      "Epoch 167 complete\n",
      "Loss was 0.22751309144496917\n",
      "\n",
      "Training\n",
      "Epoch 168 complete\n",
      "Loss was 0.2316955202817917\n",
      "\n",
      "Training\n",
      "Epoch 169 complete\n",
      "Loss was 0.22972839975357057\n",
      "\n",
      "Training\n",
      "Epoch 170 complete\n",
      "Loss was 0.23043912732601166\n",
      "\n",
      "Training\n",
      "Epoch 171 complete\n",
      "Loss was 0.22457668077945708\n",
      "\n",
      "Testing\n",
      "Test accuracy was 86.33\n",
      "\n",
      "Training\n",
      "Epoch 172 complete\n",
      "Loss was 0.23025820112228393\n",
      "\n",
      "Training\n",
      "Epoch 173 complete\n",
      "Loss was 0.22879274463653565\n",
      "\n",
      "Training\n",
      "Epoch 174 complete\n",
      "Loss was 0.2305503957271576\n",
      "\n",
      "Training\n",
      "Epoch 175 complete\n",
      "Loss was 0.22457802093029022\n",
      "\n",
      "Training\n",
      "Epoch 176 complete\n",
      "Loss was 0.2237758048772812\n",
      "\n",
      "Training\n",
      "Epoch 177 complete\n",
      "Loss was 0.22255682969093324\n",
      "\n",
      "Training\n",
      "Epoch 178 complete\n",
      "Loss was 0.2254669008255005\n",
      "\n",
      "Training\n",
      "Epoch 179 complete\n",
      "Loss was 0.2213878836631775\n",
      "\n",
      "Training\n",
      "Epoch 180 complete\n",
      "Loss was 0.22300549507141113\n",
      "\n",
      "Training\n",
      "Epoch 181 complete\n",
      "Loss was 0.22064801037311554\n",
      "\n",
      "Testing\n",
      "Test accuracy was 86.63\n",
      "\n",
      "Training\n",
      "Epoch 182 complete\n",
      "Loss was 0.2221847343444824\n",
      "\n",
      "Training\n",
      "Epoch 183 complete\n",
      "Loss was 0.220591144323349\n",
      "\n",
      "Training\n",
      "Epoch 184 complete\n",
      "Loss was 0.21979561519622803\n",
      "\n",
      "Training\n",
      "Epoch 185 complete\n",
      "Loss was 0.21780577361583708\n",
      "\n",
      "Training\n",
      "Epoch 186 complete\n",
      "Loss was 0.2212240892648697\n",
      "\n",
      "Training\n",
      "Epoch 187 complete\n",
      "Loss was 0.21779842329025267\n",
      "\n",
      "Training\n",
      "Epoch 188 complete\n",
      "Loss was 0.21819439518451691\n",
      "\n",
      "Training\n",
      "Epoch 189 complete\n",
      "Loss was 0.21726412689685823\n",
      "\n",
      "Training\n",
      "Epoch 190 complete\n",
      "Loss was 0.21646082270145417\n",
      "\n",
      "Training\n",
      "Epoch 191 complete\n",
      "Loss was 0.2202878166437149\n",
      "\n",
      "Testing\n",
      "Test accuracy was 86.47\n",
      "\n",
      "Training\n",
      "Epoch 192 complete\n",
      "Loss was 0.21953337812423707\n",
      "\n",
      "Training\n",
      "Epoch 193 complete\n",
      "Loss was 0.21274197375774384\n",
      "\n",
      "Training\n",
      "Epoch 194 complete\n",
      "Loss was 0.21551725268363953\n",
      "\n",
      "Training\n",
      "Epoch 195 complete\n",
      "Loss was 0.21659660124778748\n",
      "\n",
      "Training\n",
      "Epoch 196 complete\n",
      "Loss was 0.21248125660419465\n",
      "\n",
      "Training\n",
      "Epoch 197 complete\n",
      "Loss was 0.21536214566230774\n",
      "\n",
      "Training\n",
      "Epoch 198 complete\n",
      "Loss was 0.2123295077085495\n",
      "\n",
      "Training\n",
      "Epoch 199 complete\n",
      "Loss was 0.21390118038654327\n",
      "\n",
      "Training\n",
      "Epoch 200 complete\n",
      "Loss was 0.21325731444358825\n",
      "\n",
      "Training\n",
      "Epoch 201 complete\n",
      "Loss was 0.2170641320347786\n",
      "\n",
      "Testing\n",
      "Test accuracy was 86.74\n",
      "\n",
      "Training\n",
      "Epoch 202 complete\n",
      "Loss was 0.21261602056026457\n",
      "\n",
      "Training\n",
      "Epoch 203 complete\n",
      "Loss was 0.2151038420200348\n",
      "\n",
      "Training\n",
      "Epoch 204 complete\n",
      "Loss was 0.2114916595220566\n",
      "\n",
      "Training\n",
      "Epoch 205 complete\n",
      "Loss was 0.21008631014823914\n",
      "\n",
      "Training\n",
      "Epoch 206 complete\n",
      "Loss was 0.2140068588256836\n",
      "\n",
      "Training\n",
      "Epoch 207 complete\n",
      "Loss was 0.21215397334098815\n",
      "\n",
      "Training\n",
      "Epoch 208 complete\n",
      "Loss was 0.21264748978614806\n",
      "\n",
      "Training\n",
      "Epoch 209 complete\n",
      "Loss was 0.20902354431152342\n",
      "\n",
      "Training\n",
      "Epoch 210 complete\n",
      "Loss was 0.21152277183532714\n",
      "\n",
      "Training\n",
      "Epoch 211 complete\n",
      "Loss was 0.21090816366672516\n",
      "\n",
      "Testing\n",
      "Test accuracy was 86.2\n",
      "\n",
      "Training\n",
      "Epoch 212 complete\n",
      "Loss was 0.21129279339313506\n",
      "\n",
      "Training\n",
      "Epoch 213 complete\n",
      "Loss was 0.20512830245494842\n",
      "\n",
      "Training\n",
      "Epoch 214 complete\n",
      "Loss was 0.20706356394290923\n",
      "\n",
      "Training\n",
      "Epoch 215 complete\n",
      "Loss was 0.2107357234954834\n",
      "\n",
      "Training\n",
      "Epoch 216 complete\n",
      "Loss was 0.2145148777961731\n",
      "\n",
      "Training\n",
      "Epoch 217 complete\n",
      "Loss was 0.20877718091011047\n",
      "\n",
      "Training\n",
      "Epoch 218 complete\n",
      "Loss was 0.2095539072751999\n",
      "\n",
      "Training\n",
      "Epoch 219 complete\n",
      "Loss was 0.21155001044273378\n",
      "\n",
      "Training\n",
      "Epoch 220 complete\n",
      "Loss was 0.20247664439678192\n",
      "\n",
      "Training\n",
      "Epoch 221 complete\n",
      "Loss was 0.20734491765499116\n",
      "\n",
      "Testing\n",
      "Test accuracy was 86.69\n",
      "\n",
      "Training\n",
      "Epoch 222 complete\n",
      "Loss was 0.20365898406505584\n",
      "\n",
      "Training\n",
      "Epoch 223 complete\n",
      "Loss was 0.2052975322008133\n",
      "\n",
      "Training\n",
      "Epoch 224 complete\n",
      "Loss was 0.2086211940050125\n",
      "\n",
      "Training\n",
      "Epoch 225 complete\n",
      "Loss was 0.20576419126987458\n",
      "\n",
      "Training\n",
      "Epoch 226 complete\n",
      "Loss was 0.19886073291301729\n",
      "\n",
      "Training\n",
      "Epoch 227 complete\n",
      "Loss was 0.2020876044034958\n",
      "\n",
      "Training\n",
      "Epoch 228 complete\n",
      "Loss was 0.20500570493936537\n",
      "\n",
      "Training\n",
      "Epoch 229 complete\n",
      "Loss was 0.2091209442615509\n",
      "\n",
      "Training\n",
      "Epoch 230 complete\n",
      "Loss was 0.20253905320167542\n",
      "\n",
      "Training\n",
      "Epoch 231 complete\n",
      "Loss was 0.19990578985214233\n",
      "\n",
      "Testing\n",
      "Test accuracy was 86.06\n",
      "\n",
      "Training\n",
      "Epoch 232 complete\n",
      "Loss was 0.20426382315158845\n",
      "\n",
      "Training\n",
      "Epoch 233 complete\n",
      "Loss was 0.20413937747478486\n",
      "\n",
      "Training\n",
      "Epoch 234 complete\n",
      "Loss was 0.20770444309711455\n",
      "\n",
      "Training\n",
      "Epoch 235 complete\n",
      "Loss was 0.2026385052204132\n",
      "\n",
      "Training\n",
      "Epoch 236 complete\n",
      "Loss was 0.19849316811561585\n",
      "\n",
      "Training\n",
      "Epoch 237 complete\n",
      "Loss was 0.20136293375492095\n",
      "\n",
      "Training\n",
      "Epoch 238 complete\n",
      "Loss was 0.19772304606437682\n",
      "\n",
      "Training\n",
      "Epoch 239 complete\n",
      "Loss was 0.20631516015529633\n",
      "\n",
      "Training\n",
      "Epoch 240 complete\n",
      "Loss was 0.1978722629547119\n",
      "\n",
      "Training\n",
      "Epoch 241 complete\n",
      "Loss was 0.200267340362072\n",
      "\n",
      "Testing\n",
      "Test accuracy was 86.88\n",
      "\n",
      "Training\n",
      "Epoch 242 complete\n",
      "Loss was 0.20557621943950652\n",
      "\n",
      "Training\n",
      "Epoch 243 complete\n",
      "Loss was 0.1982159278392792\n",
      "\n",
      "Training\n",
      "Epoch 244 complete\n",
      "Loss was 0.19628623902797698\n",
      "\n",
      "Training\n",
      "Epoch 245 complete\n",
      "Loss was 0.19862108826637268\n",
      "\n",
      "Training\n",
      "Epoch 246 complete\n",
      "Loss was 0.20144598090648652\n",
      "\n",
      "Training\n",
      "Epoch 247 complete\n",
      "Loss was 0.19843424689769745\n",
      "\n",
      "Training\n",
      "Epoch 248 complete\n",
      "Loss was 0.1971889379620552\n",
      "\n",
      "Training\n",
      "Epoch 249 complete\n",
      "Loss was 0.19697217971086503\n",
      "\n",
      "Training\n",
      "Epoch 250 complete\n",
      "Loss was 0.20024374234676362\n",
      "\n",
      "Training\n",
      "Epoch 251 complete\n",
      "Loss was 0.19778747057914733\n",
      "\n",
      "Testing\n",
      "Test accuracy was 86.71\n",
      "\n",
      "Training\n",
      "Epoch 252 complete\n",
      "Loss was 0.19894732201099397\n",
      "\n",
      "Training\n",
      "Epoch 253 complete\n",
      "Loss was 0.19620880740880967\n",
      "\n",
      "Training\n",
      "Epoch 254 complete\n",
      "Loss was 0.19482183647155762\n",
      "\n",
      "Training\n",
      "Epoch 255 complete\n",
      "Loss was 0.20072052657604217\n",
      "\n",
      "Training\n",
      "Epoch 256 complete\n",
      "Loss was 0.19474455869197846\n",
      "\n",
      "Training\n",
      "Epoch 257 complete\n",
      "Loss was 0.19704269701242447\n",
      "\n",
      "Training\n",
      "Epoch 258 complete\n",
      "Loss was 0.19675350868701935\n",
      "\n",
      "Training\n",
      "Epoch 259 complete\n",
      "Loss was 0.19571913588047027\n",
      "\n",
      "Training\n",
      "Epoch 260 complete\n",
      "Loss was 0.19455671203136443\n",
      "\n",
      "Training\n",
      "Epoch 261 complete\n",
      "Loss was 0.19776434278488159\n",
      "\n",
      "Testing\n",
      "Test accuracy was 86.8\n",
      "\n",
      "Training\n",
      "Epoch 262 complete\n",
      "Loss was 0.19154663449525833\n",
      "\n",
      "Training\n",
      "Epoch 263 complete\n",
      "Loss was 0.18958288550376892\n",
      "\n",
      "Training\n",
      "Epoch 264 complete\n",
      "Loss was 0.19130225467681886\n",
      "\n",
      "Training\n",
      "Epoch 265 complete\n",
      "Loss was 0.19255536776781082\n",
      "\n",
      "Training\n",
      "Epoch 266 complete\n",
      "Loss was 0.19345911395549775\n",
      "\n",
      "Training\n",
      "Epoch 267 complete\n",
      "Loss was 0.19413331031799316\n",
      "\n",
      "Training\n",
      "Epoch 268 complete\n",
      "Loss was 0.19129425692558288\n",
      "\n",
      "Training\n",
      "Epoch 269 complete\n",
      "Loss was 0.19198710227012633\n",
      "\n",
      "Training\n",
      "Epoch 270 complete\n",
      "Loss was 0.1907101330757141\n",
      "\n",
      "Training\n",
      "Epoch 271 complete\n",
      "Loss was 0.19108848583698274\n",
      "\n",
      "Testing\n",
      "Test accuracy was 86.83\n",
      "\n",
      "Training\n",
      "Epoch 272 complete\n",
      "Loss was 0.19023570096492767\n",
      "\n",
      "Training\n",
      "Epoch 273 complete\n",
      "Loss was 0.19075604498386384\n",
      "\n",
      "Training\n",
      "Epoch 274 complete\n",
      "Loss was 0.19483745217323303\n",
      "\n",
      "Training\n",
      "Epoch 275 complete\n",
      "Loss was 0.1900678585767746\n",
      "\n",
      "Training\n",
      "Epoch 276 complete\n",
      "Loss was 0.18685510754585266\n",
      "\n",
      "Training\n",
      "Epoch 277 complete\n",
      "Loss was 0.1918855667114258\n",
      "\n",
      "Training\n",
      "Epoch 278 complete\n",
      "Loss was 0.1870776157975197\n",
      "\n",
      "Training\n",
      "Epoch 279 complete\n",
      "Loss was 0.18927602446079253\n",
      "\n",
      "Training\n",
      "Epoch 280 complete\n",
      "Loss was 0.189918148458004\n",
      "\n",
      "Training\n",
      "Epoch 281 complete\n",
      "Loss was 0.19034352457523346\n",
      "\n",
      "Testing\n",
      "Test accuracy was 86.51\n",
      "\n",
      "Training\n",
      "Epoch 282 complete\n",
      "Loss was 0.18727508318424224\n",
      "\n",
      "Training\n",
      "Epoch 283 complete\n",
      "Loss was 0.19050460815429687\n",
      "\n",
      "Training\n",
      "Epoch 284 complete\n",
      "Loss was 0.1898700013756752\n",
      "\n",
      "Training\n",
      "Epoch 285 complete\n",
      "Loss was 0.18897716081142427\n",
      "\n",
      "Training\n",
      "Epoch 286 complete\n",
      "Loss was 0.18895194172859192\n",
      "\n",
      "Training\n",
      "Epoch 287 complete\n",
      "Loss was 0.1939287155866623\n",
      "\n",
      "Training\n",
      "Epoch 288 complete\n",
      "Loss was 0.18987696319818498\n",
      "\n",
      "Training\n",
      "Epoch 289 complete\n",
      "Loss was 0.1859510202407837\n",
      "\n",
      "Training\n",
      "Epoch 290 complete\n",
      "Loss was 0.18823460644483567\n",
      "\n",
      "Training\n",
      "Epoch 291 complete\n",
      "Loss was 0.18754603600502015\n",
      "\n",
      "Testing\n",
      "Test accuracy was 86.63\n",
      "\n",
      "Training\n",
      "Epoch 292 complete\n",
      "Loss was 0.18878786718845367\n",
      "\n",
      "Training\n",
      "Epoch 293 complete\n",
      "Loss was 0.1871312257051468\n",
      "\n",
      "Training\n",
      "Epoch 294 complete\n",
      "Loss was 0.19054506242275238\n",
      "\n",
      "Training\n",
      "Epoch 295 complete\n",
      "Loss was 0.18546095323562622\n",
      "\n",
      "Training\n",
      "Epoch 296 complete\n",
      "Loss was 0.18726232212781907\n",
      "\n",
      "Training\n",
      "Epoch 297 complete\n",
      "Loss was 0.1835947287082672\n",
      "\n",
      "Training\n",
      "Epoch 298 complete\n",
      "Loss was 0.18500806206464768\n",
      "\n",
      "Training\n",
      "Epoch 299 complete\n",
      "Loss was 0.18783560001850128\n",
      "\n",
      "Training\n",
      "Epoch 300 complete\n",
      "Loss was 0.18418361365795136\n",
      "\n",
      "Training\n",
      "Epoch 301 complete\n",
      "Loss was 0.18275716906785966\n",
      "\n",
      "Testing\n",
      "Test accuracy was 86.82\n",
      "\n",
      "Training\n",
      "Epoch 302 complete\n",
      "Loss was 0.18734843182563782\n",
      "\n",
      "Training\n",
      "Epoch 303 complete\n",
      "Loss was 0.185960140645504\n",
      "\n",
      "Training\n",
      "Epoch 304 complete\n",
      "Loss was 0.18622764849662782\n",
      "\n",
      "Training\n",
      "Epoch 305 complete\n",
      "Loss was 0.18398213970661165\n",
      "\n",
      "Training\n",
      "Epoch 306 complete\n",
      "Loss was 0.18287869596481324\n",
      "\n",
      "Training\n",
      "Epoch 307 complete\n",
      "Loss was 0.1862290415763855\n",
      "\n",
      "Training\n",
      "Epoch 308 complete\n",
      "Loss was 0.18427440589666366\n",
      "\n",
      "Training\n",
      "Epoch 309 complete\n",
      "Loss was 0.18954561829566954\n",
      "\n",
      "Training\n",
      "Epoch 310 complete\n",
      "Loss was 0.18656034195423127\n",
      "\n",
      "Training\n",
      "Epoch 311 complete\n",
      "Loss was 0.18368497931957245\n",
      "\n",
      "Testing\n",
      "Test accuracy was 86.69\n",
      "\n",
      "Training\n",
      "Epoch 312 complete\n",
      "Loss was 0.18364602935314178\n",
      "\n",
      "Training\n",
      "Epoch 313 complete\n",
      "Loss was 0.18187486505508424\n",
      "\n",
      "Training\n",
      "Epoch 314 complete\n",
      "Loss was 0.18540827637910842\n",
      "\n",
      "Training\n",
      "Epoch 315 complete\n",
      "Loss was 0.1851087771654129\n",
      "\n",
      "Training\n",
      "Epoch 316 complete\n",
      "Loss was 0.1840588606595993\n",
      "\n",
      "Training\n",
      "Epoch 317 complete\n",
      "Loss was 0.17864858043193818\n",
      "\n",
      "Training\n",
      "Epoch 318 complete\n",
      "Loss was 0.18636407190561294\n",
      "\n",
      "Training\n",
      "Epoch 319 complete\n",
      "Loss was 0.18140112417936324\n",
      "\n",
      "Training\n",
      "Epoch 320 complete\n",
      "Loss was 0.18694805717468263\n",
      "\n",
      "Training\n",
      "Epoch 321 complete\n",
      "Loss was 0.17795250701904297\n",
      "\n",
      "Testing\n",
      "Test accuracy was 87.32\n",
      "\n",
      "Training\n",
      "Epoch 322 complete\n",
      "Loss was 0.17998830103874205\n",
      "\n",
      "Training\n",
      "Epoch 323 complete\n",
      "Loss was 0.18154697704315187\n",
      "\n",
      "Training\n",
      "Epoch 324 complete\n",
      "Loss was 0.1794790962934494\n",
      "\n",
      "Training\n",
      "Epoch 325 complete\n",
      "Loss was 0.18513857865333558\n",
      "\n",
      "Training\n",
      "Epoch 326 complete\n",
      "Loss was 0.17936085599660873\n",
      "\n",
      "Training\n",
      "Epoch 327 complete\n",
      "Loss was 0.17939799302816392\n",
      "\n",
      "Training\n",
      "Epoch 328 complete\n",
      "Loss was 0.18263197010755539\n",
      "\n",
      "Training\n",
      "Epoch 329 complete\n",
      "Loss was 0.1842501922249794\n",
      "\n",
      "Training\n",
      "Epoch 330 complete\n",
      "Loss was 0.17978225141763687\n",
      "\n",
      "Training\n",
      "Epoch 331 complete\n",
      "Loss was 0.1782956239581108\n",
      "\n",
      "Testing\n",
      "Test accuracy was 87.09\n",
      "\n",
      "Training\n",
      "Epoch 332 complete\n",
      "Loss was 0.18314030504226683\n",
      "\n",
      "Training\n",
      "Epoch 333 complete\n",
      "Loss was 0.17386412298679352\n",
      "\n",
      "Training\n",
      "Epoch 334 complete\n",
      "Loss was 0.17864413410425187\n",
      "\n",
      "Training\n",
      "Epoch 335 complete\n",
      "Loss was 0.17902903079986573\n",
      "\n",
      "Training\n",
      "Epoch 336 complete\n",
      "Loss was 0.1818238778114319\n",
      "\n",
      "Training\n",
      "Epoch 337 complete\n",
      "Loss was 0.17723181933164597\n",
      "\n",
      "Training\n",
      "Epoch 338 complete\n",
      "Loss was 0.1796127741932869\n",
      "\n",
      "Training\n",
      "Epoch 339 complete\n",
      "Loss was 0.17929360800981523\n",
      "\n",
      "Training\n",
      "Epoch 340 complete\n",
      "Loss was 0.17582265770435332\n",
      "\n",
      "Training\n",
      "Epoch 341 complete\n",
      "Loss was 0.17626981353759766\n",
      "\n",
      "Testing\n",
      "Test accuracy was 86.87\n",
      "\n",
      "Training\n",
      "Epoch 342 complete\n",
      "Loss was 0.17756605666875838\n",
      "\n",
      "Training\n",
      "Epoch 343 complete\n",
      "Loss was 0.17619954979419708\n",
      "\n",
      "Training\n",
      "Epoch 344 complete\n",
      "Loss was 0.18080856961011887\n",
      "\n",
      "Training\n",
      "Epoch 345 complete\n",
      "Loss was 0.17801639467477798\n",
      "\n",
      "Training\n",
      "Epoch 346 complete\n",
      "Loss was 0.18041053605079652\n",
      "\n",
      "Training\n",
      "Epoch 347 complete\n",
      "Loss was 0.17747746986150742\n",
      "\n",
      "Training\n",
      "Epoch 348 complete\n",
      "Loss was 0.1791115072965622\n",
      "\n",
      "Training\n",
      "Epoch 349 complete\n",
      "Loss was 0.17533947843313216\n",
      "\n",
      "Training\n",
      "Epoch 350 complete\n",
      "Loss was 0.17718477940559388\n",
      "\n",
      "Training\n",
      "Epoch 351 complete\n",
      "Loss was 0.1755310576558113\n",
      "\n",
      "Testing\n",
      "Test accuracy was 87.52\n",
      "\n",
      "Training\n",
      "Epoch 352 complete\n",
      "Loss was 0.17873255735635757\n",
      "\n",
      "Training\n",
      "Epoch 353 complete\n",
      "Loss was 0.17731228572130203\n",
      "\n",
      "Training\n",
      "Epoch 354 complete\n",
      "Loss was 0.1783897522687912\n",
      "\n",
      "Training\n",
      "Epoch 355 complete\n",
      "Loss was 0.17585620111227035\n",
      "\n",
      "Training\n",
      "Epoch 356 complete\n",
      "Loss was 0.18141973900794983\n",
      "\n",
      "Training\n",
      "Epoch 357 complete\n",
      "Loss was 0.17410827833414078\n",
      "\n",
      "Training\n",
      "Epoch 358 complete\n",
      "Loss was 0.17785262680053712\n",
      "\n",
      "Training\n",
      "Epoch 359 complete\n",
      "Loss was 0.17670322275161743\n",
      "\n",
      "Training\n",
      "Epoch 360 complete\n",
      "Loss was 0.17518605375289917\n",
      "\n",
      "Training\n",
      "Epoch 361 complete\n",
      "Loss was 0.1737860827445984\n",
      "\n",
      "Testing\n",
      "Test accuracy was 87.18\n",
      "\n",
      "Training\n",
      "Epoch 362 complete\n",
      "Loss was 0.17516584134101867\n",
      "\n",
      "Training\n",
      "Epoch 363 complete\n",
      "Loss was 0.17429091608524322\n",
      "\n",
      "Training\n",
      "Epoch 364 complete\n",
      "Loss was 0.1817908354997635\n",
      "\n",
      "Training\n",
      "Epoch 365 complete\n",
      "Loss was 0.17513497477769852\n",
      "\n",
      "Training\n",
      "Epoch 366 complete\n",
      "Loss was 0.17547990107536315\n",
      "\n",
      "Training\n",
      "Epoch 367 complete\n",
      "Loss was 0.1719513844847679\n",
      "\n",
      "Training\n",
      "Epoch 368 complete\n",
      "Loss was 0.17050981080532074\n",
      "\n",
      "Training\n",
      "Epoch 369 complete\n",
      "Loss was 0.17527922141551971\n",
      "\n",
      "Training\n",
      "Epoch 370 complete\n",
      "Loss was 0.17320234328508377\n",
      "\n",
      "Training\n",
      "Epoch 371 complete\n",
      "Loss was 0.1729367792606354\n",
      "\n",
      "Testing\n",
      "Test accuracy was 87.2\n",
      "\n",
      "Training\n",
      "Epoch 372 complete\n",
      "Loss was 0.17821473914384842\n",
      "\n",
      "Training\n",
      "Epoch 373 complete\n",
      "Loss was 0.17638325273990632\n",
      "\n",
      "Training\n",
      "Epoch 374 complete\n",
      "Loss was 0.17539337241649627\n",
      "\n",
      "Training\n",
      "Epoch 375 complete\n",
      "Loss was 0.1739845022559166\n",
      "\n",
      "Training\n",
      "Epoch 376 complete\n",
      "Loss was 0.17257842898368836\n",
      "\n",
      "Training\n",
      "Epoch 377 complete\n",
      "Loss was 0.17259884023666383\n",
      "\n",
      "Training\n",
      "Epoch 378 complete\n",
      "Loss was 0.17900991642475128\n",
      "\n",
      "Training\n",
      "Epoch 379 complete\n",
      "Loss was 0.17234291166067123\n",
      "\n",
      "Training\n",
      "Epoch 380 complete\n",
      "Loss was 0.1750413646697998\n",
      "\n",
      "Training\n",
      "Epoch 381 complete\n",
      "Loss was 0.1713784761428833\n",
      "\n",
      "Testing\n",
      "Test accuracy was 87.09\n",
      "\n",
      "Training\n",
      "Epoch 382 complete\n",
      "Loss was 0.17250673615932466\n",
      "\n",
      "Training\n",
      "Epoch 383 complete\n",
      "Loss was 0.17081030374765396\n",
      "\n",
      "Training\n",
      "Epoch 384 complete\n",
      "Loss was 0.1704644986987114\n",
      "\n",
      "Training\n",
      "Epoch 385 complete\n",
      "Loss was 0.1731554937362671\n",
      "\n",
      "Training\n",
      "Epoch 386 complete\n",
      "Loss was 0.1750836665034294\n",
      "\n",
      "Training\n",
      "Epoch 387 complete\n",
      "Loss was 0.17508971482515334\n",
      "\n",
      "Training\n",
      "Epoch 388 complete\n",
      "Loss was 0.17499285393953323\n",
      "\n",
      "Training\n",
      "Epoch 389 complete\n",
      "Loss was 0.1687252743244171\n",
      "\n",
      "Training\n",
      "Epoch 390 complete\n",
      "Loss was 0.1708759054541588\n",
      "\n",
      "Training\n",
      "Epoch 391 complete\n",
      "Loss was 0.17228943264484406\n",
      "\n",
      "Testing\n",
      "Test accuracy was 87.37\n",
      "\n",
      "Training\n",
      "Epoch 392 complete\n",
      "Loss was 0.16961784899234772\n",
      "\n",
      "Training\n",
      "Epoch 393 complete\n",
      "Loss was 0.1723651664853096\n",
      "\n",
      "Training\n",
      "Epoch 394 complete\n",
      "Loss was 0.1705530155301094\n",
      "\n",
      "Training\n",
      "Epoch 395 complete\n",
      "Loss was 0.1713257305622101\n",
      "\n",
      "Training\n",
      "Epoch 396 complete\n",
      "Loss was 0.1734107848405838\n",
      "\n",
      "Training\n",
      "Epoch 397 complete\n",
      "Loss was 0.17579065871238708\n",
      "\n",
      "Training\n",
      "Epoch 398 complete\n",
      "Loss was 0.16924122828245164\n",
      "\n",
      "Training\n",
      "Epoch 399 complete\n",
      "Loss was 0.1708880639076233\n",
      "\n",
      "Training\n",
      "Epoch 400 complete\n",
      "Loss was 0.16316537529230118\n",
      "\n",
      "Training\n",
      "Epoch 401 complete\n",
      "Loss was 0.16925198650360107\n",
      "\n",
      "Testing\n",
      "Test accuracy was 87.68\n",
      "\n",
      "Training\n",
      "Epoch 402 complete\n",
      "Loss was 0.17274998927116395\n",
      "\n",
      "Training\n",
      "Epoch 403 complete\n",
      "Loss was 0.1750409641265869\n",
      "\n",
      "Training\n",
      "Epoch 404 complete\n",
      "Loss was 0.16615061157941818\n",
      "\n",
      "Training\n",
      "Epoch 405 complete\n",
      "Loss was 0.169638867855072\n",
      "\n",
      "Training\n",
      "Epoch 406 complete\n",
      "Loss was 0.1730041915178299\n",
      "\n",
      "Training\n",
      "Epoch 407 complete\n",
      "Loss was 0.1680260427594185\n",
      "\n",
      "Training\n",
      "Epoch 408 complete\n",
      "Loss was 0.16875794845819472\n",
      "\n",
      "Training\n",
      "Epoch 409 complete\n",
      "Loss was 0.17434817504882813\n",
      "\n",
      "Training\n",
      "Epoch 410 complete\n",
      "Loss was 0.16603275549411772\n",
      "\n",
      "Training\n",
      "Epoch 411 complete\n",
      "Loss was 0.16709974020719529\n",
      "\n",
      "Testing\n",
      "Test accuracy was 86.67\n",
      "\n",
      "Training\n",
      "Epoch 412 complete\n",
      "Loss was 0.174944988489151\n",
      "\n",
      "Training\n",
      "Epoch 413 complete\n",
      "Loss was 0.17061929351091384\n",
      "\n",
      "Training\n",
      "Epoch 414 complete\n",
      "Loss was 0.17047121995687484\n",
      "\n",
      "Training\n",
      "Epoch 415 complete\n",
      "Loss was 0.16673753410577774\n",
      "\n",
      "Training\n",
      "Epoch 416 complete\n",
      "Loss was 0.17140363931655883\n",
      "\n",
      "Training\n",
      "Epoch 417 complete\n",
      "Loss was 0.1665513916015625\n",
      "\n",
      "Training\n",
      "Epoch 418 complete\n",
      "Loss was 0.17097002255916596\n",
      "\n",
      "Training\n",
      "Epoch 419 complete\n",
      "Loss was 0.1738950698375702\n",
      "\n",
      "Training\n",
      "Epoch 420 complete\n",
      "Loss was 0.16931182193756103\n",
      "\n",
      "Training\n",
      "Epoch 421 complete\n",
      "Loss was 0.16972404330968857\n",
      "\n",
      "Testing\n",
      "Test accuracy was 87.6\n",
      "\n",
      "Training\n",
      "Epoch 422 complete\n",
      "Loss was 0.16896197366714477\n",
      "\n",
      "Training\n",
      "Epoch 423 complete\n",
      "Loss was 0.16509914994239808\n",
      "\n",
      "Training\n",
      "Epoch 424 complete\n",
      "Loss was 0.17009692996740342\n",
      "\n",
      "Training\n",
      "Epoch 425 complete\n",
      "Loss was 0.16885152477025986\n",
      "\n",
      "Training\n",
      "Epoch 426 complete\n",
      "Loss was 0.16892916405200958\n",
      "\n",
      "Training\n",
      "Epoch 427 complete\n",
      "Loss was 0.1711052936911583\n",
      "\n",
      "Training\n",
      "Epoch 428 complete\n",
      "Loss was 0.16961589097976684\n",
      "\n",
      "Training\n",
      "Epoch 429 complete\n",
      "Loss was 0.16617265218496322\n",
      "\n",
      "Training\n",
      "Epoch 430 complete\n",
      "Loss was 0.1675730213522911\n",
      "\n",
      "Training\n",
      "Epoch 431 complete\n",
      "Loss was 0.16821830135583876\n",
      "\n",
      "Testing\n",
      "Test accuracy was 87.41\n",
      "\n",
      "Training\n",
      "Epoch 432 complete\n",
      "Loss was 0.1670300218462944\n",
      "\n",
      "Training\n",
      "Epoch 433 complete\n",
      "Loss was 0.1668823267221451\n",
      "\n",
      "Training\n",
      "Epoch 434 complete\n",
      "Loss was 0.17359893637895585\n",
      "\n",
      "Training\n",
      "Epoch 435 complete\n",
      "Loss was 0.16338124161958695\n",
      "\n",
      "Training\n",
      "Epoch 436 complete\n",
      "Loss was 0.1675023517012596\n",
      "\n",
      "Training\n",
      "Epoch 437 complete\n",
      "Loss was 0.1663168222308159\n",
      "\n",
      "Training\n",
      "Epoch 438 complete\n",
      "Loss was 0.16555197978019714\n",
      "\n",
      "Training\n",
      "Epoch 439 complete\n",
      "Loss was 0.16514920288324356\n",
      "\n",
      "Training\n",
      "Epoch 440 complete\n",
      "Loss was 0.1692692358493805\n",
      "\n",
      "Training\n",
      "Epoch 441 complete\n",
      "Loss was 0.1703197527527809\n",
      "\n",
      "Testing\n",
      "Test accuracy was 87.41\n",
      "\n",
      "Training\n",
      "Epoch 442 complete\n",
      "Loss was 0.16750319796800614\n",
      "\n",
      "Training\n",
      "Epoch 443 complete\n",
      "Loss was 0.16940888756513595\n",
      "\n",
      "Training\n",
      "Epoch 444 complete\n",
      "Loss was 0.16453417599201203\n",
      "\n",
      "Training\n",
      "Epoch 445 complete\n",
      "Loss was 0.1642880020737648\n",
      "\n",
      "Training\n",
      "Epoch 446 complete\n",
      "Loss was 0.16893443715572357\n",
      "\n",
      "Training\n",
      "Epoch 447 complete\n",
      "Loss was 0.16412298500537872\n",
      "\n",
      "Training\n",
      "Epoch 448 complete\n",
      "Loss was 0.1656866102218628\n",
      "\n",
      "Training\n",
      "Epoch 449 complete\n",
      "Loss was 0.16900800269842148\n",
      "\n",
      "Training\n",
      "Epoch 450 complete\n",
      "Loss was 0.16323031264543533\n",
      "\n",
      "Training\n",
      "Epoch 451 complete\n",
      "Loss was 0.16582913082838058\n",
      "\n",
      "Testing\n",
      "Test accuracy was 86.64\n",
      "\n",
      "Training\n",
      "Epoch 452 complete\n",
      "Loss was 0.16402465718984605\n",
      "\n",
      "Training\n",
      "Epoch 453 complete\n",
      "Loss was 0.16739437639713287\n",
      "\n",
      "Training\n",
      "Epoch 454 complete\n",
      "Loss was 0.16568279641866684\n",
      "\n",
      "Training\n",
      "Epoch 455 complete\n",
      "Loss was 0.17032170498371124\n",
      "\n",
      "Training\n",
      "Epoch 456 complete\n",
      "Loss was 0.16430074143409729\n",
      "\n",
      "Training\n",
      "Epoch 457 complete\n",
      "Loss was 0.17282820230722426\n",
      "\n",
      "Training\n",
      "Epoch 458 complete\n",
      "Loss was 0.1643931804895401\n",
      "\n",
      "Training\n",
      "Epoch 459 complete\n",
      "Loss was 0.16396676671504976\n",
      "\n",
      "Training\n",
      "Epoch 460 complete\n",
      "Loss was 0.16544884669780732\n",
      "\n",
      "Training\n",
      "Epoch 461 complete\n",
      "Loss was 0.16765557146072388\n",
      "\n",
      "Testing\n",
      "Test accuracy was 87.62\n",
      "\n",
      "Training\n",
      "Epoch 462 complete\n",
      "Loss was 0.1633236842751503\n",
      "\n",
      "Training\n",
      "Epoch 463 complete\n",
      "Loss was 0.16292911273241042\n",
      "\n",
      "Training\n",
      "Epoch 464 complete\n",
      "Loss was 0.16474314045906066\n",
      "\n",
      "Training\n",
      "Epoch 465 complete\n",
      "Loss was 0.16461971431970596\n",
      "\n",
      "Training\n",
      "Epoch 466 complete\n",
      "Loss was 0.16908136582374572\n",
      "\n",
      "Training\n",
      "Epoch 467 complete\n",
      "Loss was 0.1618946853876114\n",
      "\n",
      "Training\n",
      "Epoch 468 complete\n",
      "Loss was 0.1633031188249588\n",
      "\n",
      "Training\n",
      "Epoch 469 complete\n",
      "Loss was 0.16409890979528427\n",
      "\n",
      "Training\n",
      "Epoch 470 complete\n",
      "Loss was 0.16420019847154618\n",
      "\n",
      "Training\n",
      "Epoch 471 complete\n",
      "Loss was 0.1655391710996628\n",
      "\n",
      "Testing\n",
      "Test accuracy was 87.43\n",
      "\n",
      "Training\n",
      "Epoch 472 complete\n",
      "Loss was 0.16138811308145523\n",
      "\n",
      "Training\n",
      "Epoch 473 complete\n",
      "Loss was 0.16195884370803834\n",
      "\n",
      "Training\n",
      "Epoch 474 complete\n",
      "Loss was 0.16747391331195832\n",
      "\n",
      "Training\n",
      "Epoch 475 complete\n",
      "Loss was 0.1627260663509369\n",
      "\n",
      "Training\n",
      "Epoch 476 complete\n",
      "Loss was 0.15904299879074096\n",
      "\n",
      "Training\n",
      "Epoch 477 complete\n",
      "Loss was 0.1632112196087837\n",
      "\n",
      "Training\n",
      "Epoch 478 complete\n",
      "Loss was 0.16016325533390044\n",
      "\n",
      "Training\n",
      "Epoch 479 complete\n",
      "Loss was 0.15964751601219176\n",
      "\n",
      "Training\n",
      "Epoch 480 complete\n",
      "Loss was 0.164279419362545\n",
      "\n",
      "Training\n",
      "Epoch 481 complete\n",
      "Loss was 0.16063984912633897\n",
      "\n",
      "Testing\n",
      "Test accuracy was 87.36\n",
      "\n",
      "Training\n",
      "Epoch 482 complete\n",
      "Loss was 0.16319864636659623\n",
      "\n",
      "Training\n",
      "Epoch 483 complete\n",
      "Loss was 0.16291478216648103\n",
      "\n",
      "Training\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_16920\\777580125.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     26\u001B[0m     \u001B[1;31m# backwards\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     27\u001B[0m     \u001B[0moptimizer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mzero_grad\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 28\u001B[1;33m     \u001B[0mloss\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     29\u001B[0m     \u001B[0moptimizer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstep\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     30\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\torch\\_tensor.py\u001B[0m in \u001B[0;36mbackward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    361\u001B[0m                 \u001B[0mcreate_graph\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mcreate_graph\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    362\u001B[0m                 inputs=inputs)\n\u001B[1;32m--> 363\u001B[1;33m         \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mautograd\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mgradient\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mretain_graph\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcreate_graph\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minputs\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0minputs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    364\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    365\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mregister_hook\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mhook\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\torch\\autograd\\__init__.py\u001B[0m in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    173\u001B[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001B[0;32m    174\u001B[0m         \u001B[0mtensors\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mgrad_tensors_\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mretain_graph\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcreate_graph\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minputs\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 175\u001B[1;33m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001B[0m\u001B[0;32m    176\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    177\u001B[0m def grad(\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "confusion_matrix = torch.zeros(10, 10)\n",
    "with torch.no_grad():\n",
    "    for i, (inputs, classes) in enumerate(test_loader):\n",
    "        inputs = inputs.to(device)\n",
    "        classes = classes.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        for t, p in zip(classes.view(-1), preds.view(-1)):\n",
    "                confusion_matrix[t.long(), p.long()] += 1\n",
    "\n",
    "test_accuracy_per_class = pd.DataFrame(confusion_matrix.diag()/confusion_matrix.sum(1), test_data.classes, [\"Accuracy\"])\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.grid(color='black', axis='y', linestyle='dashed', zorder = 1)\n",
    "bar = plt.bar(test_accuracy_per_class.index, height=test_accuracy_per_class.Accuracy*100, zorder = 2)\n",
    "for rect in bar:\n",
    "    height = rect.get_height()\n",
    "    plt.text(rect.get_x() + rect.get_width() / 2.0, height, f'{height:.2f}%', ha='center', va='bottom', zorder = 3)\n",
    "plt.yticks(range(0, 101, 5))\n",
    "plt.axhline((confusion_matrix.diag()/confusion_matrix.sum(1)).mean()*100, color='r', linestyle='--')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def baseline_func(input):\n",
    "  return 0\n",
    "\n",
    "visualizer = AttributionVisualizer(\n",
    "    models=[CNN],\n",
    "    score_func=lambda o: torch.nn.Dropout ,\n",
    "    features=[\n",
    "        ImageFeature(\n",
    "            \"Photo\",\n",
    "            baseline_transforms=[baseline_func],\n",
    "            input_transforms=[test_transform],\n",
    "        )\n",
    "    ],\n",
    "    dataset=test_data,\n",
    ")\n",
    "visualizer.render()"
   ],
   "metadata": {
    "id": "GamKBAqPiyS4",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 1440x720 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'classes'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_16920\\924025805.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     37\u001B[0m         )\n\u001B[0;32m     38\u001B[0m     ],\n\u001B[1;32m---> 39\u001B[1;33m     \u001B[0mdataset\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mtest_data\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     40\u001B[0m )\n\u001B[0;32m     41\u001B[0m \u001B[0mvisualizer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mrender\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mTypeError\u001B[0m: __init__() missing 1 required positional argument: 'classes'"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "visualizer.serve()"
   ],
   "metadata": {
    "id": "bYyMlOlfiyV6",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# get some random training images\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# create grid of images\n",
    "img_grid = torchvision.utils.make_grid(images)\n",
    "\n",
    "# show images\n",
    "matplotlib_imshow(img_grid, one_channel=True)\n",
    "\n",
    "# write to tensorboard\n",
    "writer.add_graph(model_copy, images)\n",
    "writer.add_image('first_visualization_test', img_grid)\n",
    "writer.close()"
   ],
   "metadata": {
    "id": "bGHX3IX3iyYc",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir runs/first_visualization_test/"
   ],
   "metadata": {
    "id": "A-g1rugJiybL",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}