For this task, we decided to use a convolutional network since these have shown a lot of promise in image classification in recent years (citation needed). We used 5 different types of layers, each with their own purpose, to help the model classify images. A convolutional layer works by applying a moving filter over the input which can help to identify features and patterns such as edges. A max pooling layer reduces the size of the input by reducing a cluster of values to only their highest value. Batch normalisation layers normalise the data so that it has a mean of 0 and a standard deviation of 1 which helps networks converge faster due to the effect it has on the activation function (citation needed). The dropout layers randomly set values to 0 to reduce overfitting and linear layers are the standard layer used in a neural network and in this case it helps go from the convolutional layers to some form of usable output. After experimenting with numerous loss functions, optimizers and activation functions, we found the most success using cross entropy loss, adam optimizer and ReLu activation. To help reduce overfitting, we applied numerous transformations to the input data. These were RandomHorizontalFlip and RandomAffine which would flip some of the images and shift them across randomly upto 10%. We also normalised the data so it was easier for the network to manipulate it. 
During this project, we were limited by the memory and processing power available to us. While we were trying various networks, we ran them on Google Collab and one of our computers. We were also given the stipulation that we arenâ€™t allowed to train a network continuously for more than 24 hours. To speed up the process of training the neural network, we made sure to take advantage of a GPU wherever possible.
