Here we outline the architecture of the Convolutional Neural Network we designed in order to attempt to accurately classify images. The integral layers of a convolutional neural network are its convolutional layers of which our network has 7. The first layer is the input layer in the form of a convolutional layer which has 3 input channels. This layer works by applying a moving filter over the input which can help to identify features and patterns such as edges. A max pooling layer is then used which reduces the size of the input by reducing a cluster of values to only their highest value. This has the effect of not only reducing the computational load due to the lowering of the image resolution but may also help to reduce overfitting. After this the network then utilises dropout to probabilistically drop out nodes in the layer output at a probability of 0.3. This has the effect of again, reducing the overfitting of the network and was chosen as it is more effective than other standard computationally inexpensive regularisation [1 p265]. The network then repeats this pattern of Convolutional layer, followed by a max pool followed by a dropout. Next the network employs a batch normalisation which adjusts the values such that the mean and standard deviation stay close to 0 and 1 respectively. It then employs another block of convolution, maxPool and dropout. Then it uses another 3 layers of convolution with a batch normalisation between the first and second and a dropout between the second and third to reduce overtraining once again. Lastly linear layers are used to transform the output of the convolutional layers down to 10 features such that each feature corresponds to a label in the original dataset. The randomized leaky rectified linear unit function (Randomized ReLU) is applied to all of the convolutional layers and all but the final linear layer as their activation function. The ReLU function has gained popularity in recent years in the deep learning domain and the Randomized ReLU is nothing but an improved version of the ReLU function. (WHY?).RELU EQUATION
