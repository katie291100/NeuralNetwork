Here we outline the architecture of the Convolutional Neural Network we designed in order to attempt to accurately classify images as seen in Fig 1. The integral layers of a convolutional neural network are its convolutional layers of which our network has 7. These work by .......... Our architecture works by putting other layers inbetween each convolutional layer to help combat the effects of overfitting and strain on resources. Between each convolutional layer has atleast one of either max pooling, drop out or batch normalisation layers. Each one causing an increase in the maximum accuracy achieved before the model overfits or accuracy plataus. Max pooling was applied after the first, second and fourth convolutional layer to ensure that the network generalises the location of features from the offset, meaning that it is more accurate for testing images. It highlights the most important features in each patch of the image by calculating the largest values which means that the feature maps are less exact on where the location of each feature in the image must be. Adding more of these max pooling layers meant that the model seemed to over generalise and the accuracy plataud quicker but without them the input of the fully connected layer near the end of the network would have to be 230400 which is too large to feasibly be able to train on our hardware. Drop outs are applied with an increasing probablistic rate of 0.1 - 0.4 following convolutional layers 2, 3, 4 and 5. This randomly ignores some of the outputs of the layer with a set probability of choosing the nodes. The dropout probability rates were kept small as we decided we wanted to avoid making our network too wide and increasing the time in which it would take to train as a side effect of using drop out is a network may become sparse [4]  causing the need for a wider network. Furthermore, increasing the drop out rate has the drawback of increasing training time overall as the network is training on a very different data set in each layer. [4]  Batch normalisation was added between layers 2, 5 and 6 which works to works to stabalise the learning by standardising the inputs to a layer by reducing the internal covariate shift by whitening the inputs to each layer.[5] This fixes the distribution of the layer inputs and improves the training speed. The output of the final convolutional layer is flattened and passed into the final layer which are the fully connected layers which means that every node in the previous layer connects to every node in the following layer. Each fully connected layer steps down the output until it reaches 10 features, each representing a label that it predicts an input to be a part of.  The randomized leaky rectified linear unit function (Randomized ReLU) is applied to all of the convolutional layers and all but the fully connected linear layer as their activation function. The ReLU function seems to have gained popularity in recent which is why it was chosen as our activation function, most notibly AlexNet uses ReLu [ALEXNET]. 
 Cross entropy loss is applied to the output of the network after each batch, this is equivalent to applying softmax followed by the negative log likelihood loss function which is used by the optimiser to train the network. This was chosen because it is a standard loss function and its use greatly improves performance of models with softmax outputs [1]. PyTorch uses the form of the equation \ell(x, y) = L = {l_1, ..., l_N}^T, \ \ l_n = -w_{y_n} log \frac{exp(x_{n, y_n})}{\Sigma^C_{c=1} exp(x_{n,c})} \cdot 1
 where x is the individual input, y is the output, w is the weight and C is the number of classes, in this case 10. The loss function is run on a batch, giving the output L = { l1​, … ,lN​ }⊤ where N is equivalent to the batch size. The main basis of how cross entropy loss works is that given a set of inputs and outputs (xi, yi), the functions is able to assign the probability that xi is to be assigned to each class [3] given class indices [0, C].
To increase the generalisation of the network which helps increase accuracy on the testing set, and to again reduce overfitting, image preprocessing is used for the testing data. A random affine is applied to the test set which randomly applies a horizontal and vertical shift by upto ± 10% and sets the blank space to a default value of 0. The images are also randomly flipped horizontally. The images are also normalised to the values of the calculated mean and standard deviation of the CIFAR10 dataset, which are [0.4914, 0.4822, 0.4465] and [0.2470, 0.2435, 0.2616] respectively.
The optimiser the network uses is the Adam optimiser which is a method for efficient stochastic optimization that only requires first-order gradients with little memory requirement [2] which was important considering our hardware constraints. The optimiser adjusts the weights such that the loss of the training set is minised. Adam optimiser implements a weight decay of which is set to 0.0015 which prevents the weights from increasing infinitely and helps to reduce overfitting. The learning rate for the optimiser is XXXX, this was chosen due to a range of experiments and was increased from 0.00015 due to the optimiser appearing to get stuck in a local minima. The batch size has been set to 200 which is due to the trade off between speed and accuracy EXPLAIN MORE.
