Here we outline the architecture of the Convolutional Neural Network we designed in order to attempt to accurately classify images. The integral layers of a convolutional neural network are its convolutional layers of which our network has 7. The first layer is the input layer in the form of a convolutional layer which has 3 input channels. This layer works by applying a moving filter over the input which can help to identify features and patterns such as edges. A max pooling layer is then used which reduces the size of the input by reducing a cluster of values to only their highest value. This has the effect of not only reducing the computational load due to the lowering of the image resolution but may also help to reduce overfitting. After this the network then utilises dropout to probabilistically drop out nodes in the layer output at a probability of 0.3. This has the effect of again, reducing the overfitting of the network and was chosen as it is more effective than other standard computationally inexpensive regularisation [1 p265]. The network then repeats this pattern of Convolutional layer, followed by a max pool followed by a dropout. Next the network employs a batch normalisation which adjusts the values such that the mean and standard deviation stay close to 0 and 1 respectively. It then employs another block of convolution, maxPool and dropout. Then it uses another 3 layers of convolution with a batch normalisation between the first and second and a dropout between the second and third to reduce overtraining once again. Lastly linear layers are used to transform the output of the convolutional layers down to 10 features such that each feature corresponds to a label in the original dataset. The randomized leaky rectified linear unit function (Randomized ReLU) is applied to all of the convolutional layers and all but the final linear layer as their activation function. The ReLU function has gained popularity in recent years in the deep learning domain and the Randomized ReLU is nothing but an improved version of the ReLU function.

RELU EQUATION

 Cross entropy loss is applied to the output of the network after each batch, this is equivalent to applying softmax followed by the negative log likelihood loss function which is used by the optimiser to train the network. This was chosen because it is a standard loss function and its use greatly improves performance of models with softmax outputs [1].   
 
Cross Entropy Loss Function 

To increase the generalisation of the network and to again reduce overfitting, image preprocessing is used for the testing data. A random affine is applied to the test set which randomly applies a horizontal and vertical shift by upto Â± 10%  and sets the blank space to a default value of 0. The images are also randomly flipped horizontally. The images are also normalised to the values of the mean and standard deviation of the CIFAR10 dataset, which are [0.4914, 0.4822, 0.4465] and [0.2470, 0.2435, 0.2616] respectively. 

The optimiser the network uses is the Adam optimiser which is a method for efficient stochastic optimization that only requires first-order gradients with little memory requirement [2] which was important considering our hardware constraints. Adam optimiser implements a weight decay of which is set to 0.0015 which prevents the weights from increasing infinitely and helps to reduce overfitting. The learning rate for the optimiser is **XXXX**, this was chosen due to a range of experiments and was increased from 0.00015 due to the optimiser appearing to get stuck in a local minima. The batch size has been set to **XXX** which is due to the trade off between speed and accuracy **EXPLAIN MORE**. 


[1] Goodfellow, I., Courville, A., Bengio, Y. (2016). Deep Learning. United Kingdom: MIT Press.
[2] https://arxiv.org/pdf/1412.6980.pdf Adam optimiser
